{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "import torchsde\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import lightning as L\n",
    "\n",
    "from utils_7 import CV_params_prior_mu, CV_params_prior_sigma, CV_params\n",
    "\n",
    "import torch\n",
    "import torchsde\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "class PhysiologicalSDE_batched(torchsde.SDEIto):\n",
    "    def __init__(self, sigma, sigma_tx, params, confounder_type,non_confounded_effect):\n",
    "        super(PhysiologicalSDE_batched, self).__init__(noise_type=\"diagonal\")\n",
    "        self.params = params\n",
    "        self.sigma = sigma\n",
    "        self.sigma_tx = sigma_tx\n",
    "        self.confounder_type = confounder_type\n",
    "        self.non_confounded_effect = non_confounded_effect\n",
    "\n",
    "    def calculate_diext_dt(self,t, batch_size):\n",
    "        # Assuming 't' might be a tensor with the shape [batch_size] or a scalar\n",
    "        # Ensure 't' is at least 1D tensor with the shape [batch_size]\n",
    "        t = t * torch.ones(batch_size)  # This line is only necessary if t might be a scalar\n",
    "\n",
    "        # Calculate factor\n",
    "        factor = -2 * (t - 5) / 5\n",
    "        # Calculate exponential\n",
    "        exponential = torch.exp(-((t - 5) / 5) ** 2)\n",
    "\n",
    "        # Calculate diext_dt and ensure it's of shape [batch x 1]\n",
    "        diext_dt = 5 * factor * exponential\n",
    "\n",
    "        return diext_dt.unsqueeze(1)\n",
    "    \n",
    "    def create_treatment_effect(self):\n",
    "        #print('creating treatment effect')\n",
    "        if self.confounder_type == 'visible':\n",
    "            #confound on the initial NORMALISED PA - STATIC CONFOUNDER (not time dep)\n",
    "            initp_transform  = 0.5+(self.params[\"confounding_pressure\"]-0.75)/0.1\n",
    "            ##print('initp_transform', initp_transform.shape)\n",
    "            A_ = self.v_fun(initp_transform, scale = 0.02)\n",
    "        elif self.confounder_type == 'partial':\n",
    "            #confound on the initial NORMALISED SV - STATIC CONFOUNDER (not time dep)\n",
    "            init_sv_transform  = 0.5+(self.params[\"confounding_sv\"]-0.58)/0.1 \n",
    "            #print('init_sv_transform', init_sv_transform[0])\n",
    "            A_ = self.v_fun(init_sv_transform, scale = 0.3)\n",
    "        elif self.confounder_type == 'invisible':\n",
    "            A_ = self.v_fun(self.params[\"confounder_random_number\"])\n",
    "        elif self.confounder_type == 'none':\n",
    "            A_ = torch.tensor([1])\n",
    "            \n",
    "        self.treatment_effect = A_\n",
    "        return A_\n",
    "\n",
    "    def v_fun(self, x, scale):\n",
    "        cos_term = torch.cos(5 * x - 0.2)\n",
    "        square_term = (5 - x) ** 2\n",
    "        return scale * (cos_term * square_term) ** 2\n",
    "\n",
    "    def f(self, t, y):\n",
    "        \n",
    "        if abs(t.item() - 40.0) < 0.001:\n",
    "            ##print(t)\n",
    "            #print('saving params')\n",
    "            self.params[\"confounding_pressure\"] = y[:, 0]\n",
    "            self.params[\"confounding_sv\"] = y[:, 3]\n",
    "            self.params[\"cv\"] = torch.rand_like(y[:, 0]) * 100 + 10 if self.non_confounded_effect else self.params[\"cv\"] \n",
    "            self.params[\"confounder_random_number\"] = torch.rand_like(y[:, 0])\n",
    "            #print('confounding_pressure', y[:, 0])\n",
    "            #print('confounding_sv', y[:, 3])\n",
    "            A_ = self.create_treatment_effect()\n",
    "        \n",
    "        elif t.item() <40.0:\n",
    "            A_ = torch.tensor([0])\n",
    "\n",
    "        elif t.item()>40.0:\n",
    "            A_ = self.create_treatment_effect()\n",
    "\n",
    "        p_a = 100. * y[:, 0].unsqueeze(1)\n",
    "        p_v = 100. * y[:, 1].unsqueeze(1)\n",
    "        s = y[:, 2].unsqueeze(1)\n",
    "        sv = 100. * y[:, 3].unsqueeze(1)\n",
    "        i_ext = y[:, 4].unsqueeze(1)\n",
    "        batch_size = y.shape[0]\n",
    "\n",
    "        #print('A_ ', A_.shape)\n",
    "        i_ext_tx_effect = A_.unsqueeze(1) * i_ext\n",
    "        \n",
    "        #print('time, i_ext pre, A_, i_ext_post, pv, sv', t.item(),i_ext[0].item(), A_[0], i_ext_tx_effect[0].item(), p_v[0].item(), sv[0].item())   \n",
    "\n",
    "        f_hr = s * (self.params[\"f_hr_max\"] - self.params[\"f_hr_min\"]) + self.params[\"f_hr_min\"]\n",
    "        r_tpr = s * (self.params[\"r_tpr_max\"] - self.params[\"r_tpr_min\"]) + self.params[\"r_tpr_min\"] + self.params[\"r_tpr_mod\"]\n",
    "\n",
    "        dva_dt = -1. * (p_a - p_v) / r_tpr + sv * f_hr\n",
    "        dvv_dt = -1. * dva_dt + i_ext_tx_effect\n",
    "        dpa_dt = dva_dt / (self.params[\"ca\"] * 100.)\n",
    "        dpv_dt = dvv_dt / (self.params[\"cv\"] * 10.)\n",
    "        ds_dt = (1. / self.params[\"tau\"]) * (1. - 1. / (1 + torch.exp(-self.params[\"k_width\"] * (p_a - self.params[\"p_aset\"]))) - s)\n",
    "        dsv_dt = i_ext_tx_effect * self.params[\"sv_mod\"]\n",
    "\n",
    "        if self.params[\"treatment\"] and (t >= self.params[\"t_treatment\"]):\n",
    "            #note that the \n",
    "            time_since_treatment = t - self.params[\"t_treatment\"]\n",
    "            diext_dt = self.calculate_diext_dt(time_since_treatment, batch_size)\n",
    "            diext_dt = torch.relu(i_ext + diext_dt) - i_ext \n",
    "        else:\n",
    "            #diext_dt = torch.full((batch_size, 1), 1)\n",
    "            diext_dt = torch.zeros_like(dpa_dt)\n",
    "\n",
    "        ##print('dpa_dt, dpv_dt, ds_dt, dsv_dt, diext_dt',dpa_dt.shape, dpv_dt.shape, ds_dt.shape, dsv_dt.shape, diext_dt.shape )\n",
    "        \n",
    "        diff_res = torch.concat([dpa_dt, dpv_dt, ds_dt, dsv_dt, diext_dt], dim=-1)\n",
    "        #print('diff_results example', diff_res[0,:])\n",
    "\n",
    "        ##print('diff_res', diff_res.shape)\n",
    "        return diff_res\n",
    "    \n",
    "    def g(self, t, y):\n",
    "        diffusion = torch.full_like(y, self.sigma)\n",
    "        if self.params[\"treatment\"] and (t >= self.params[\"t_treatment\"]):\n",
    "            diffusion[:, 4] = self.sigma_tx\n",
    "        return diffusion\n",
    "\n",
    "\n",
    "\n",
    "def scale_numbers(x, original_min, original_max, target_min, target_max):\n",
    "    return target_min + ((target_max - target_min) * (x - original_min) / (original_max - original_min))\n",
    "\n",
    "\n",
    "def init_random_state():\n",
    "    max_ves = 64.0 - 10.0\n",
    "    min_ves = 36.0 + 10.0\n",
    "\n",
    "    max_ved = 167.0 - 10.0\n",
    "    min_ved = 121.0 + 10.0\n",
    "\n",
    "    max_pa = 100.0\n",
    "    min_pa = 85.0\n",
    "\n",
    "    max_pv = 70.0\n",
    "    min_pv = 30.0\n",
    "\n",
    "    max_s = 0.05\n",
    "    min_s = 0.01\n",
    "\n",
    "    max_sv = 90\n",
    "    min_sv = 83\n",
    "\n",
    "    init_ves = (np.random.rand() * (max_ves - min_ves) + min_ves) / 100.0\n",
    "    # init_ves = 50.0 / 100.0\n",
    "\n",
    "    init_ved = (np.random.rand() * (max_ved - min_ved) + min_ved) / 100.0\n",
    "    # init_ved = 144.0 / 100.0\n",
    "\n",
    "    init_pa = (np.random.rand() * (max_pa - min_pa) + min_pa) / 100.0\n",
    "    init_pv = (np.random.rand() * (max_pv - min_pv) + min_pv) / 100.0\n",
    "    init_s = (np.random.rand() * (max_s - min_s) + min_s)\n",
    "    init_sv = (np.random.rand() * (max_sv - min_sv) + min_sv) / 100.0\n",
    "\n",
    "    init_i_ext = 0\n",
    "\n",
    "    init_state = np.array([init_pa, init_pv, init_s, init_sv, init_i_ext])\n",
    "    ##print('init_state', init_state)\n",
    "    return init_state\n",
    "\n",
    "\n",
    "def create_cv_data(include_all_inputs, N,gamma,noise_std, sigma_tx, r_tpr_mod, confounder_type, non_confounded_effect, t_span, t_treatment, t_cutoff, seed, post_treatment_dims, pre_treatment_dims, normalize = False):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    X = []\n",
    "    Y_0 = []\n",
    "    Y_1 = []\n",
    "    init_state_list = []\n",
    "    \n",
    "    params = {\"r_tpr_mod\": r_tpr_mod, #the mod is in case we want to simulate decreasing the total peripheral resistance i.e. shock\n",
    "            \"f_hr_max\": 3.0,\n",
    "            \"f_hr_min\": 2.0 / 3.0,\n",
    "            \"r_tpr_max\": 2.134,\n",
    "            \"r_tpr_min\": 0.5335,\n",
    "            \"sv_mod\": 0.001,  ## this is also added on from the original model to simulate effect of fluid directly on the stroke volume\n",
    "            \"ca\": 4.0,\n",
    "            \"cv\": 111.0,\n",
    "\n",
    "            # dS/dt parameters\n",
    "            \"k_width\": 0.1838,\n",
    "            \"p_aset\": 70,\n",
    "            \"tau\": 20,\n",
    "            \"t_treatment\" : t_treatment,\n",
    "            }\n",
    "    \n",
    "    output_params = {\"r_tpr_mod\": r_tpr_mod, #the mod is in case we want to simulate decreasing the total peripheral resistance i.e. shock\n",
    "            \"f_hr_max\": 3.0,\n",
    "            \"f_hr_min\": 2.0 / 3.0,\n",
    "            \"r_tpr_max\": 2.134,\n",
    "            \"r_tpr_min\": 0.5335,\n",
    "            \"sv_mod\": 0.001,  ## this is also added on from the original model to simulate effect of fluid directly on the stroke volume\n",
    "            \"ca\": 4.0,\n",
    "            \"cv\": 111.0,\n",
    "\n",
    "            # dS/dt parameters\n",
    "            \"k_width\": 0.1838,\n",
    "            \"p_aset\": 70,\n",
    "            \"tau\": 20\n",
    "            }\n",
    "    \n",
    "    params_treatment = params.copy()\n",
    "    params_treatment[\"treatment\"]=True\n",
    "    params_notreatment = params.copy()\n",
    "    params_notreatment[\"treatment\"]=False\n",
    "\n",
    "    params_for_output = params.copy()\n",
    "    \n",
    "\n",
    "    t = np.arange(t_span).astype(float)\n",
    "    t_tensor = torch.tensor(t, dtype=torch.float32)\n",
    "\n",
    "    #print(f\"N: {N}\")\n",
    "    #print(f\"Gamma: {gamma}\")\n",
    "    #print(f\"Noise Std: {noise_std}\")\n",
    "    #print(f\"Sigma Tx: {sigma_tx}\")\n",
    "    #print(f\"Confounder Type: {confounder_type}\")\n",
    "    #print(f\"Non Confounded Effect: {non_confounded_effect}\")\n",
    "    #print(f\"t_span: {t_span}\")\n",
    "    #print(f\"t_treatment: {t_treatment}\")\n",
    "    #print(f\"Seed: {seed}\")\n",
    "    #print(f\"Post Treatment Dims: {post_treatment_dims}\")\n",
    "    #print(f\"Pre Treatment Dims: {pre_treatment_dims}\")\n",
    "    #print(f\"Normalize: {normalize}\")\n",
    "\n",
    "    #print('creating initial random states for both treated and untreated ')\n",
    "    init_state_list = []\n",
    "    for i in range(N):\n",
    "        init_state_list.append(init_random_state())\n",
    "    init_state_tensor = torch.tensor(np.array(init_state_list), dtype=torch.float32)\n",
    "\n",
    "    #creating treated from those initial random states\n",
    "    #print('creating treated')\n",
    "    print('init_state_tensor', init_state_tensor.shape, 't_tensor', t_tensor.shape, )\n",
    "    sde = PhysiologicalSDE_batched(sigma = noise_std, sigma_tx=sigma_tx, confounder_type=confounder_type , non_confounded_effect = non_confounded_effect, params=params_treatment)\n",
    "    Y_1 = torchsde.sdeint(sde, init_state_tensor, t_tensor, method='euler',dt=0.05).squeeze(1)\n",
    "\n",
    "    #print('creating untreated')\n",
    "    #created untreated from the same initial random satates\n",
    "    sde = PhysiologicalSDE_batched(sigma = noise_std, sigma_tx=sigma_tx, confounder_type=confounder_type , non_confounded_effect = non_confounded_effect, params=params_notreatment)\n",
    "    Y_0 = torchsde.sdeint(sde, init_state_tensor, t_tensor, method='euler', dt=0.05).squeeze(1)\n",
    "\n",
    "    print('Y0 Y1', Y_0.shape, Y_1.shape)\n",
    "    X = Y_0[t_cutoff, :, :]\n",
    "    init_state = Y_0[t_cutoff, :, :] #the confounder is not based on the 'initial' values but rather the ones just before treatment \n",
    "        \n",
    "    #print('Assigning confounding factors')\n",
    "    ##print('init_state', init_state.shape)\n",
    "    if confounder_type == 'visible':\n",
    "        scaled_pa = scale_numbers(x=init_state[:, 0], original_min=0.85, original_max=1.0, target_min=0, target_max=1)\n",
    "        p = torch.sigmoid(gamma*scaled_pa) # use the arterial pressure as visible confounder\n",
    "    \n",
    "    elif confounder_type == 'partial':# use the stroke volume as a partially visible confounder\n",
    "        scaled_sv = scale_numbers(x=init_state[:, 3], original_min=0.83, original_max=0.9, target_min=0, target_max=1)\n",
    "        p  = torch.sigmoid(gamma*scaled_sv) \n",
    "\n",
    "    elif confounder_type == 'invisible':\n",
    "        p =  torch.sigmoid(gamma*torch.rand(N)) \n",
    "\n",
    "    ##print('p', p.shape)\n",
    "    T = torch.zeros(N)\n",
    "    #T determines which trajectories as selected as treated (overlap level is controlled by gamma)\n",
    "    T[torch.rand(N)<p] = 1\n",
    "\n",
    "    #all_trajectories = torch.cat([Y_0, Y_1], dim = 1)\n",
    "    Y_0 = Y_0[:, :, :4].permute(1, 0, 2)  # drop the dim used to create I-external and permute to [batch, seq_len, dim]\n",
    "    Y_1 = Y_1[:, :, :4].permute(1, 0, 2)\n",
    "    T_expanded = T[:, None, None]\n",
    "    #print('Y0, Y1, T_expanded', Y_0.shape, Y_1.shape, T_expanded.shape)\n",
    "    print('Y0 Y1 permuted', Y_0.shape, Y_1.shape)\n",
    "    # the 'factual' trajectories are the UNtreated outcome (Y0) for the not Treated (1-T) and the Treated outcome (Y1) for the factually Treated (T)\n",
    "    Y_fact = Y_0 * (1-T_expanded)+ Y_1 *T_expanded\n",
    "\n",
    "    # the 'COUNTERfactual' trajectories are the UNtreated outcome (Y0) for the Treated (T) and the Treated outcome (Y1) for the factually UNtreated (1-T)\n",
    "    # we would never actually have access to the counterfactual other than in this situation where we are simulating it \n",
    "    Y_cf = Y_0 * T_expanded + Y_1 * (1-T_expanded)\n",
    "\n",
    "    print('Y_fact Y_cf', Y_fact.shape, Y_cf.shape)\n",
    "    Y_fact = Y_fact[:, t_cutoff:, :]\n",
    "    Y_cf = Y_cf[:, t_cutoff:, :]\n",
    "\n",
    "    Y_fact_np = Y_fact.detach().cpu().numpy()\n",
    "    states_mean = Y_fact_np.mean(axis=(0, 1))\n",
    "    states_min = Y_fact_np.min(axis=(0, 1))\n",
    "    states_max = Y_fact_np.max(axis=(0, 1))\n",
    "    #print('states_mean', states_mean, 'states_min', states_min, 'states_max', states_max)\n",
    "\n",
    "    Y_fact_until_t = Y_fact[:, :t_treatment-t_cutoff, :]\n",
    "    print('Y_fact_until_t', Y_fact_until_t.shape)\n",
    "    mu = Y_fact_until_t.mean((0,1))\n",
    "    std = Y_fact_until_t.std((0,1))\n",
    "    print('mu', mu.shape)\n",
    "    print('std', std.shape)\n",
    "    \n",
    "    if normalize:\n",
    "        Y_fact = (Y_fact - mu)/std\n",
    "        Y_cf = (Y_cf - mu)/std\n",
    "        mu_X = X.mean([0,1])\n",
    "        std_X = X.std([0,1])\n",
    "        X = (X-mu_X)/std_X\n",
    "\n",
    "    CV_params_prior_mu['pa'] = mu[0]*100\n",
    "    CV_params_prior_mu['pv'] = mu[1] *100\n",
    "    CV_params_prior_mu['s'] = mu[2]\n",
    "    CV_params_prior_mu['sv'] = mu[3] *100\n",
    "\n",
    "    CV_params_prior_sigma['pa'] = std[0]*100\n",
    "    CV_params_prior_sigma['pv'] = std[1]*100\n",
    "    CV_params_prior_sigma['s'] = std[2]\n",
    "    CV_params_prior_sigma['sv'] = std[3]*100\n",
    "\n",
    "    CV_params['max_pa'] = (mu[0] + 2.5 * std[0]) * 100\n",
    "    CV_params['min_pa'] = (mu[0] - 2.5 * std[0]) * 100\n",
    "    CV_params['max_pv'] = (mu[1] + 2.5 * std[1]) * 100\n",
    "    CV_params['min_pv'] = (mu[1] - 2.5 * std[1]) * 100\n",
    "    CV_params['max_s'] = (mu[2] + 2.5 * std[2]) \n",
    "    CV_params['min_s'] = (mu[2] - 2.5 * std[2]) \n",
    "    CV_params['max_sv'] = (mu[3] + 2.5 * std[3]) * 100\n",
    "    CV_params['min_sv'] = (mu[3] - 2.5 * std[3]) * 100\n",
    "\n",
    "    \n",
    "    # Now split these factual and counterfactual trajectories by the 'before' and 'after treatment' so we have a baseline \n",
    "    \n",
    "    pre_treat_mask = (t<=t_treatment)[t_cutoff:]\n",
    "    post_treat_mask = (t>t_treatment)[t_cutoff:]\n",
    "\n",
    "    print('pre_treat_mask', pre_treat_mask.shape)\n",
    "    print('post_treat_mask', post_treat_mask.shape)\n",
    "\n",
    "    # We define X as the Factual trajectory BEFORE treatment, and X_ as the COUNTERfactual traj BEFORE treatment \n",
    "    X_static = X\n",
    "    if include_all_inputs:\n",
    "        X = Y_fact[:,pre_treat_mask]\n",
    "    else:\n",
    "        X = Y_fact[:,pre_treat_mask][:,:,pre_treatment_dims]\n",
    "        X_ = Y_cf[:,pre_treat_mask][:,:,pre_treatment_dims]\n",
    "\n",
    "    print('X', X.shape)\n",
    "\n",
    "    # We redfine Y_fact as the Factual trajectory AFTER treatment, and Y_cf as the COUNTERfactual traj AFTER treatment \n",
    "    # We are selecting the DIASTOLIC BP (output dim = 1) as the one to maintain.. this is because the fluid is only really affecting this within the time values\n",
    "    full_fact_traj = Y_fact\n",
    "    full_CF_traj = Y_cf\n",
    "\n",
    "    print('full_fact_traj', full_fact_traj.shape)\n",
    "    print('full_CF_traj', full_CF_traj.shape)\n",
    "\n",
    "\n",
    "    Y_fact = Y_fact[:,post_treat_mask][:,:,post_treatment_dims]\n",
    "    Y_cf = Y_cf[:,post_treat_mask][:,:,post_treatment_dims]\n",
    "\n",
    "    print('Y_fact', Y_fact.shape)\n",
    "    print('Y_cf',Y_cf.shape)\n",
    "\n",
    "    # we split the time vector also as before and after treatment \n",
    "    t_new = np.arange(t_span - t_cutoff)\n",
    "    t_x = t_new[pre_treat_mask]\n",
    "    t_y = t_new[post_treat_mask]\n",
    "\n",
    "    print('t_x', t_x.shape)\n",
    "    print('t_y', t_y.shape)\n",
    "    # and get it to match the dimensions, so it's not a vector t, but a matrix of dimensions N by before_tx and N by after_tx\n",
    "    t_X = torch.Tensor(np.tile(t_x[None,:],(X.shape[0],1)))\n",
    "    t_Y = torch.Tensor(np.tile(t_y[None,:],(Y_fact.shape[0],1))) - t_x[-1]\n",
    "    t_FULL = torch.tensor(t_new).unsqueeze(0).repeat(X.shape[0], 1)\n",
    "    expert_ODE_size = X.shape[1] - 1 #need to remove the i_ext which is only for creating data not models\n",
    "\n",
    "    if include_all_inputs:\n",
    "        params_tensor = torch.tensor(list(params_for_output.values()), dtype=torch.float32)  # Shape: [11]\n",
    "        params_tensor = params_tensor.reshape(1, 1, -1)\n",
    "        print('params_tensor', params_tensor.shape)\n",
    "        params_tensor_X = params_tensor.expand(N, X.shape[1], -1)[:, :, :-1] #remove t_treatment\n",
    "        params_tensor_full = params_tensor.expand(N, full_fact_traj.shape[1], -1)[:, :, :-1] #remove t_treatment\n",
    "        print('params_tensor_X', params_tensor_X.shape)\n",
    "        print('params_tensor_full', params_tensor_full.shape)\n",
    "\n",
    "        full_fact_traj = torch.cat([full_fact_traj, params_tensor_full], dim=-1)\n",
    "        full_CF_traj = torch.cat([full_fact_traj, params_tensor_full], dim=-1)\n",
    "        X = torch.cat([X, params_tensor_X], dim=-1)  \n",
    "\n",
    "\n",
    "    return X, X_static, T, Y_fact, Y_cf, p, init_state, t_X, t_Y, expert_ODE_size, t_FULL, full_fact_traj,full_CF_traj, sde\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_load_save_data(dataset_params, data_path):\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "        print(f\"Created directory: {data_path}\")\n",
    "\n",
    "    include_all = 'T' if dataset_params['include_all_inputs'] else 'F'\n",
    "    non_confounded_effect_str = 'T' if dataset_params['non_confounded_effect'] else 'F'\n",
    "    normalize = 'T' if dataset_params['normalize'] else 'F'\n",
    "    post_treatment_dims_str = ''.join(map(str, dataset_params['post_treatment_dims']))\n",
    "    pre_treatment_dims_str = ''.join(map(str, dataset_params['pre_treatment_dims']))\n",
    "\n",
    "    # Create a descriptive file name\n",
    "    filename = f\"allIn{include_all}_{dataset_params['confounder_type']}_RCE{non_confounded_effect_str}_N{dataset_params['N']}_G{dataset_params['gamma']}_Dstd{dataset_params['noise_std']}_Tstd{dataset_params['sigma_tx']}_Pre{pre_treatment_dims_str}_Post{post_treatment_dims_str}_Norm{normalize}_Rtpr{dataset_params['r_tpr_mod']}.pt\"\n",
    "    final_data_path = os.path.join(data_path, filename)\n",
    "\n",
    "    if os.path.exists(final_data_path):\n",
    "        print(\"Loading existing dataset.\")\n",
    "        data = torch.load(final_data_path)\n",
    "\n",
    "    else:\n",
    "        print(\"Creating and saving a new dataset.\")\n",
    "        # Generate data\n",
    "        X, X_static, T, Y_fact, Y_cf, p, init_state, t_X, t_Y, expert_ODE_size, t_FULL, full_fact_traj,full_CF_traj, sde = create_cv_data(include_all_inputs = dataset_params['include_all_inputs'], \n",
    "                                                                                                                                          N = dataset_params['N'],\n",
    "                                                                                                                                      gamma = dataset_params['gamma'],\n",
    "                                                                                                                                      noise_std = dataset_params['noise_std'], \n",
    "                                                                                                                                      r_tpr_mod = dataset_params['r_tpr_mod'],\n",
    "                                                                                                                                      sigma_tx = dataset_params['sigma_tx'], \n",
    "                                                                                                                                      confounder_type = dataset_params['confounder_type'], \n",
    "                                                                                                                                      non_confounded_effect = dataset_params['non_confounded_effect'], \n",
    "                                                                                                                                      t_span = dataset_params['t_span'], \n",
    "                                                                                                                                      t_treatment = dataset_params['t_treatment'], \n",
    "                                                                                                                                      t_cutoff = dataset_params['t_cutoff'],\n",
    "                                                                                                                                      seed = dataset_params['seed'], \n",
    "                                                                                                                                      post_treatment_dims = dataset_params['post_treatment_dims'], \n",
    "                                                                                                                                      pre_treatment_dims = dataset_params['pre_treatment_dims'], \n",
    "                                                                                                                                      normalize = dataset_params['normalize'])\n",
    "    \n",
    "        data = {'X': X, 'T': T,'Y_fact': Y_fact, 'Y_cf': Y_cf,'p': p,'init_state': init_state,'t_X': t_X,'t_Y': t_Y,'t_full': t_FULL,'full_fact_traj': full_fact_traj,'full_CF_traj': full_CF_traj }\n",
    "        # Save the dataset\n",
    "        torch.save(data, final_data_path)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CVDataset_loaded(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # Unpack the data\n",
    "        self.X = data['X']\n",
    "        self.T = data['T']\n",
    "        self.Y_fact = data['Y_fact']\n",
    "        self.Y_cf = data['Y_cf']\n",
    "        self.p = data['p']\n",
    "        self.init_state = data['init_state']\n",
    "        self.t_X = data['t_X']\n",
    "        self.t_Y = data['t_Y']\n",
    "        self.t_full = data['t_full']\n",
    "        self.full_fact_traj = data['full_fact_traj']\n",
    "        self.full_CF_traj = data['full_CF_traj']\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y_fact[idx], self.T[idx], self.Y_cf[idx], self.p[idx], self.init_state[idx],self.t_X[idx], self.t_Y[idx], self.t_full[idx], self.full_fact_traj[idx],self.full_CF_traj[idx]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CVDataModule_final(L.LightningDataModule):\n",
    "    def __init__(self, train_val_data, OOD_test_data,  batch_size=32, num_workers =1):\n",
    "        super().__init__()\n",
    "        self.train_val_data = train_val_data\n",
    "        self.OOD_test_data = OOD_test_data\n",
    "        self.batch_size = batch_size\n",
    "        self.dataset = None\n",
    "        self.num_workers = num_workers\n",
    "        self.encoder_input_dim = train_val_data['X'].shape[-1]\n",
    "        self.expert_latent_dim = train_val_data['full_fact_traj'].shape[-1]\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        # Load the dataset\n",
    "        self.dataset_train_val = CVDataset_loaded(self.train_val_data)\n",
    "        self.dataset_OOD_test_data = CVDataset_loaded(self.OOD_test_data)\n",
    "        \n",
    "        dataset_size = len(self.dataset_train_val)\n",
    "        train_size = int(0.8 * dataset_size)  \n",
    "        train_idx = np.arange(train_size)  \n",
    "        val_idx = np.arange(train_size, dataset_size) \n",
    "\n",
    "        # Create subsets\n",
    "        self.train = Subset(self.dataset_train_val, train_idx)\n",
    "        self.val = Subset(self.dataset_train_val, val_idx)\n",
    "\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True,\n",
    "            drop_last=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=True, \n",
    "            drop_last=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.dataset_OOD_test_data,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            drop_last=False,\n",
    "            pin_memory=True\n",
    "            )\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and saving a new dataset.\n",
      "init_state_tensor torch.Size([1280, 5]) t_tensor torch.Size([60])\n",
      "Y0 Y1 torch.Size([60, 1280, 5]) torch.Size([60, 1280, 5])\n",
      "Y0 Y1 permuted torch.Size([1280, 60, 4]) torch.Size([1280, 60, 4])\n",
      "Y_fact Y_cf torch.Size([1280, 60, 4]) torch.Size([1280, 60, 4])\n",
      "Y_fact_until_t torch.Size([1280, 5, 4])\n",
      "mu torch.Size([4])\n",
      "std torch.Size([4])\n",
      "pre_treat_mask (20,)\n",
      "post_treat_mask (20,)\n",
      "X torch.Size([1280, 6, 4])\n",
      "full_fact_traj torch.Size([1280, 20, 4])\n",
      "full_CF_traj torch.Size([1280, 20, 4])\n",
      "Y_fact torch.Size([1280, 14, 1])\n",
      "Y_cf torch.Size([1280, 14, 1])\n",
      "t_x (6,)\n",
      "t_y (14,)\n",
      "params_tensor torch.Size([1, 1, 12])\n",
      "params_tensor_X torch.Size([1280, 6, 11])\n",
      "params_tensor_full torch.Size([1280, 20, 11])\n",
      "Creating and saving a new dataset.\n",
      "init_state_tensor torch.Size([1280, 5]) t_tensor torch.Size([60])\n",
      "Y0 Y1 torch.Size([60, 1280, 5]) torch.Size([60, 1280, 5])\n",
      "Y0 Y1 permuted torch.Size([1280, 60, 4]) torch.Size([1280, 60, 4])\n",
      "Y_fact Y_cf torch.Size([1280, 60, 4]) torch.Size([1280, 60, 4])\n",
      "Y_fact_until_t torch.Size([1280, 5, 4])\n",
      "mu torch.Size([4])\n",
      "std torch.Size([4])\n",
      "pre_treat_mask (20,)\n",
      "post_treat_mask (20,)\n",
      "X torch.Size([1280, 6, 4])\n",
      "full_fact_traj torch.Size([1280, 20, 4])\n",
      "full_CF_traj torch.Size([1280, 20, 4])\n",
      "Y_fact torch.Size([1280, 14, 1])\n",
      "Y_cf torch.Size([1280, 14, 1])\n",
      "t_x (6,)\n",
      "t_y (14,)\n",
      "params_tensor torch.Size([1, 1, 12])\n",
      "params_tensor_X torch.Size([1280, 6, 11])\n",
      "params_tensor_full torch.Size([1280, 20, 11])\n"
     ]
    }
   ],
   "source": [
    "dataset_params = {\n",
    "        'include_all_inputs':True, \n",
    "        'gamma': 10,\n",
    "        'sigma_tx': 2,\n",
    "        'confounder_type': 'partial',\n",
    "\n",
    "        'non_confounded_effect': False,\n",
    "        'noise_std': 0.0,\n",
    "        't_span': 60,\n",
    "        't_treatment': 45,\n",
    "        't_cutoff':40,\n",
    "        'seed': 1,\n",
    "        'pre_treatment_dims': [0, 1],\n",
    "        'post_treatment_dims': [0],\n",
    "        'normalize': False,\n",
    "        'N': 1280\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "data_path = '/Users/riccardoconci/Local_documents/ACS submissions/THESIS/Core_paper_implementations/cf-ode/causalode/data_rc'\n",
    "#data_path = '/Users/riccardoconci/Local_documents/ACS submissions/THESIS/Counterfactual_ICU/Version_7/data_created'\n",
    "dataset_params['r_tpr_mod'] = 0.0\n",
    "train_val_data = create_load_save_data(dataset_params, data_path)\n",
    "dataset_params['r_tpr_mod'] = -0.5 \n",
    "test_data = create_load_save_data(dataset_params, data_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_state_tensor torch.Size([1000, 5]) t_tensor torch.Size([60])\n",
      "Y0 Y1 torch.Size([60, 1000, 5]) torch.Size([60, 1000, 5])\n",
      "Y0 Y1 permuted torch.Size([1000, 60, 4]) torch.Size([1000, 60, 4])\n",
      "Y_fact Y_cf torch.Size([1000, 60, 4]) torch.Size([1000, 60, 4])\n",
      "Y_fact_until_t torch.Size([1000, 5, 4])\n",
      "mu torch.Size([4])\n",
      "std torch.Size([4])\n",
      "pre_treat_mask (20,)\n",
      "post_treat_mask (20,)\n",
      "X torch.Size([1000, 6, 4])\n",
      "full_fact_traj torch.Size([1000, 20, 4])\n",
      "full_CF_traj torch.Size([1000, 20, 4])\n",
      "Y_fact torch.Size([1000, 14, 1])\n",
      "Y_cf torch.Size([1000, 14, 1])\n",
      "t_x (6,)\n",
      "t_y (14,)\n",
      "params_tensor torch.Size([1, 1, 12])\n",
      "params_tensor_X torch.Size([1000, 6, 11])\n",
      "params_tensor_full torch.Size([1000, 20, 11])\n"
     ]
    }
   ],
   "source": [
    "dataset_params = {\n",
    "        'include_all_inputs':True, \n",
    "        'gamma': 0,\n",
    "        'sigma_tx': 2,\n",
    "        'confounder_type': 'partial',\n",
    "        'r_tpr_mod':1.0, #for train! - lower for test \n",
    "\n",
    "        'non_confounded_effect': False,\n",
    "        'noise_std': 0.0,\n",
    "        't_span': 60,\n",
    "        't_treatment': 45,\n",
    "        't_cutoff':40,\n",
    "        'seed': 1,\n",
    "        'pre_treatment_dims': [0, 1],\n",
    "        'post_treatment_dims': [0],\n",
    "        'normalize': False,\n",
    "        'N': 1000\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "X, X_static, T, Y_fact, Y_cf, p, init_state, t_X, t_Y, expert_ODE_size, t_FULL, full_fact_traj,full_CF_traj, sde = create_cv_data(include_all_inputs = dataset_params['include_all_inputs'], \n",
    "                                                                                                                                          N = dataset_params['N'],\n",
    "                                                                                                                                      gamma = dataset_params['gamma'],\n",
    "                                                                                                                                      noise_std = dataset_params['noise_std'], \n",
    "                                                                                                                                      r_tpr_mod = dataset_params['r_tpr_mod'],\n",
    "                                                                                                                                      sigma_tx = dataset_params['sigma_tx'], \n",
    "                                                                                                                                      confounder_type = dataset_params['confounder_type'], \n",
    "                                                                                                                                      non_confounded_effect = dataset_params['non_confounded_effect'], \n",
    "                                                                                                                                      t_span = dataset_params['t_span'], \n",
    "                                                                                                                                      t_treatment = dataset_params['t_treatment'], \n",
    "                                                                                                                                      t_cutoff = dataset_params['t_cutoff'],\n",
    "                                                                                                                                      seed = dataset_params['seed'], \n",
    "                                                                                                                                      post_treatment_dims = dataset_params['post_treatment_dims'], \n",
    "                                                                                                                                      pre_treatment_dims = dataset_params['pre_treatment_dims'], \n",
    "                                                                                                                                      normalize = dataset_params['normalize'])\n",
    "\n",
    "\n",
    "trainval_data = {'X': X, 'T': T,'Y_fact': Y_fact, 'Y_cf': Y_cf,'p': p,'init_state': init_state,'t_X': t_X,'t_Y': t_Y,'t_full': t_FULL,'full_fact_traj': full_fact_traj,'full_CF_traj': full_CF_traj }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init_state_tensor torch.Size([1000, 5]) t_tensor torch.Size([60])\n",
      "Y0 Y1 torch.Size([60, 1000, 5]) torch.Size([60, 1000, 5])\n",
      "Y0 Y1 permuted torch.Size([1000, 60, 4]) torch.Size([1000, 60, 4])\n",
      "Y_fact Y_cf torch.Size([1000, 60, 4]) torch.Size([1000, 60, 4])\n",
      "Y_fact_until_t torch.Size([1000, 5, 4])\n",
      "mu torch.Size([4])\n",
      "std torch.Size([4])\n",
      "pre_treat_mask (20,)\n",
      "post_treat_mask (20,)\n",
      "X torch.Size([1000, 6, 4])\n",
      "full_fact_traj torch.Size([1000, 20, 4])\n",
      "full_CF_traj torch.Size([1000, 20, 4])\n",
      "Y_fact torch.Size([1000, 14, 1])\n",
      "Y_cf torch.Size([1000, 14, 1])\n",
      "t_x (6,)\n",
      "t_y (14,)\n",
      "params_tensor torch.Size([1, 1, 12])\n",
      "params_tensor_X torch.Size([1000, 6, 11])\n",
      "params_tensor_full torch.Size([1000, 20, 11])\n"
     ]
    }
   ],
   "source": [
    "dataset_params = {\n",
    "        'include_all_inputs':True, \n",
    "        'gamma': 0,\n",
    "        'sigma_tx': 2,\n",
    "        'confounder_type': 'partial',\n",
    "        'r_tpr_mod':0.8, #for train! - lower for test \n",
    "\n",
    "        'non_confounded_effect': False,\n",
    "        'noise_std': 0.0,\n",
    "        't_span': 60,\n",
    "        't_treatment': 45,\n",
    "        't_cutoff':40,\n",
    "        'seed': 1,\n",
    "        'pre_treatment_dims': [0, 1],\n",
    "        'post_treatment_dims': [0],\n",
    "        'normalize': False,\n",
    "        'N': 1280\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "X, X_static, T, Y_fact, Y_cf, p, init_state, t_X, t_Y, expert_ODE_size, t_FULL, full_fact_traj,full_CF_traj, sde = create_cv_data(include_all_inputs = dataset_params['include_all_inputs'], \n",
    "                                                                                                                                          N = dataset_params['N'],\n",
    "                                                                                                                                      gamma = dataset_params['gamma'],\n",
    "                                                                                                                                      noise_std = dataset_params['noise_std'], \n",
    "                                                                                                                                      r_tpr_mod = dataset_params['r_tpr_mod'],\n",
    "                                                                                                                                      sigma_tx = dataset_params['sigma_tx'], \n",
    "                                                                                                                                      confounder_type = dataset_params['confounder_type'], \n",
    "                                                                                                                                      non_confounded_effect = dataset_params['non_confounded_effect'], \n",
    "                                                                                                                                      t_span = dataset_params['t_span'], \n",
    "                                                                                                                                      t_treatment = dataset_params['t_treatment'], \n",
    "                                                                                                                                      t_cutoff = dataset_params['t_cutoff'],\n",
    "                                                                                                                                      seed = dataset_params['seed'], \n",
    "                                                                                                                                      post_treatment_dims = dataset_params['post_treatment_dims'], \n",
    "                                                                                                                                      pre_treatment_dims = dataset_params['pre_treatment_dims'], \n",
    "                                                                                                                                      normalize = dataset_params['normalize'])\n",
    "\n",
    "\n",
    "test_data = {'X': X, 'T': T,'Y_fact': Y_fact, 'Y_cf': Y_cf,'p': p,'init_state': init_state,'t_X': t_X,'t_Y': t_Y,'t_full': t_FULL,'full_fact_traj': full_fact_traj,'full_CF_traj': full_CF_traj }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_data_module = CVDataModule_final(train_val_data = trainval_data, OOD_test_data = test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_data_module.input_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompTest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
